{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa, librosa.display\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.transforms import transforms\n",
    "from sklearn.preprocessing import StandardScaler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Relevant Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VoxCeleb1 ID</th>\n",
       "      <th>VGGFace1 ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id10001</td>\n",
       "      <td>A.J._Buckley</td>\n",
       "      <td>m</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id10002</td>\n",
       "      <td>A.R._Rahman</td>\n",
       "      <td>m</td>\n",
       "      <td>India</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id10003</td>\n",
       "      <td>Aamir_Khan</td>\n",
       "      <td>m</td>\n",
       "      <td>India</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id10004</td>\n",
       "      <td>Aaron_Tveit</td>\n",
       "      <td>m</td>\n",
       "      <td>USA</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id10005</td>\n",
       "      <td>Aaron_Yoo</td>\n",
       "      <td>m</td>\n",
       "      <td>USA</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>id11247</td>\n",
       "      <td>Zachary_Levi</td>\n",
       "      <td>m</td>\n",
       "      <td>USA</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>id11248</td>\n",
       "      <td>Zachary_Quinto</td>\n",
       "      <td>m</td>\n",
       "      <td>USA</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>id11249</td>\n",
       "      <td>Zack_Snyder</td>\n",
       "      <td>m</td>\n",
       "      <td>USA</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>id11250</td>\n",
       "      <td>Zoe_Saldana</td>\n",
       "      <td>f</td>\n",
       "      <td>USA</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>id11251</td>\n",
       "      <td>Zulay_Henao</td>\n",
       "      <td>f</td>\n",
       "      <td>USA</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1211 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     VoxCeleb1 ID     VGGFace1 ID Gender Nationality  Set\n",
       "0         id10001    A.J._Buckley      m     Ireland  dev\n",
       "1         id10002     A.R._Rahman      m       India  dev\n",
       "2         id10003      Aamir_Khan      m       India  dev\n",
       "3         id10004     Aaron_Tveit      m         USA  dev\n",
       "4         id10005       Aaron_Yoo      m         USA  dev\n",
       "...           ...             ...    ...         ...  ...\n",
       "1206      id11247    Zachary_Levi      m         USA  dev\n",
       "1207      id11248  Zachary_Quinto      m         USA  dev\n",
       "1208      id11249     Zack_Snyder      m         USA  dev\n",
       "1209      id11250     Zoe_Saldana      f         USA  dev\n",
       "1210      id11251     Zulay_Henao      f         USA  dev\n",
       "\n",
       "[1211 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = pd.read_csv(r'C:\\Users\\sally\\Documents\\Fall 2020\\CIS 519 - Intro to Machine Learning\\Project\\audioclassification_meta.csv')\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r'C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav'\n",
    "filenames = []\n",
    "for foldername in os.listdir(directory):\n",
    "    folder_dir = os.path.join(directory,foldername)\n",
    "    for subfoldername in os.listdir(folder_dir):\n",
    "        subfolder_dir = os.path.join(folder_dir,subfoldername)\n",
    "        for filename in os.listdir(subfolder_dir):\n",
    "            file = os.path.join(subfolder_dir,filename)\n",
    "            filenames.append((foldername,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = pd.DataFrame(filenames)\n",
    "files.rename(columns={0:'ID',1:'file'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_df = meta.merge(files,left_on='VoxCeleb1 ID',right_on='ID')[['ID','Gender','Nationality','file']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id10001</td>\n",
       "      <td>m</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id10001</td>\n",
       "      <td>m</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id10001</td>\n",
       "      <td>m</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id10001</td>\n",
       "      <td>m</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id10001</td>\n",
       "      <td>m</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148637</th>\n",
       "      <td>id11251</td>\n",
       "      <td>f</td>\n",
       "      <td>USA</td>\n",
       "      <td>C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148638</th>\n",
       "      <td>id11251</td>\n",
       "      <td>f</td>\n",
       "      <td>USA</td>\n",
       "      <td>C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148639</th>\n",
       "      <td>id11251</td>\n",
       "      <td>f</td>\n",
       "      <td>USA</td>\n",
       "      <td>C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148640</th>\n",
       "      <td>id11251</td>\n",
       "      <td>f</td>\n",
       "      <td>USA</td>\n",
       "      <td>C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148641</th>\n",
       "      <td>id11251</td>\n",
       "      <td>f</td>\n",
       "      <td>USA</td>\n",
       "      <td>C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id11...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148642 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID Gender Nationality  \\\n",
       "0       id10001      m     Ireland   \n",
       "1       id10001      m     Ireland   \n",
       "2       id10001      m     Ireland   \n",
       "3       id10001      m     Ireland   \n",
       "4       id10001      m     Ireland   \n",
       "...         ...    ...         ...   \n",
       "148637  id11251      f         USA   \n",
       "148638  id11251      f         USA   \n",
       "148639  id11251      f         USA   \n",
       "148640  id11251      f         USA   \n",
       "148641  id11251      f         USA   \n",
       "\n",
       "                                                     file  \n",
       "0       C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id10...  \n",
       "1       C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id10...  \n",
       "2       C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id10...  \n",
       "3       C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id10...  \n",
       "4       C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id10...  \n",
       "...                                                   ...  \n",
       "148637  C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id11...  \n",
       "148638  C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id11...  \n",
       "148639  C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id11...  \n",
       "148640  C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id11...  \n",
       "148641  C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav\\id11...  \n",
       "\n",
       "[148642 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain list of nationalities and count of gender\n",
    "nationality_gender_count=pd.DataFrame(wav_df.groupby('Nationality')['Gender'].nunique()).reset_index()\n",
    "nationality_gender_count=nationality_gender_count[nationality_gender_count['Gender'] > 1]['Nationality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only nationalities with samples from both genders\n",
    "wav_df=wav_df.merge(nationality_gender_count,left_on='Nationality',right_on='Nationality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of nationalities\n",
    "listNat=wav_df['Nationality'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cap samples at 500 max\n",
    "columns=['ID', 'Gender', 'Nationality','file']\n",
    "samples = pd.DataFrame(columns=columns)\n",
    "for i in range(len(listNat)):\n",
    "    total = wav_df[wav_df['Nationality'] == listNat[i]].count()['ID']\n",
    "    num = int(0.5 * total)\n",
    "    if num < 500:\n",
    "        samp = wav_df[wav_df['Nationality'] == listNat[i]].sample(num,random_state=42)\n",
    "    else:\n",
    "        samp = wav_df[wav_df['Nationality'] == listNat[i]].sample(500,random_state=42)\n",
    "    samples = samples.append(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nationality</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Australia</th>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Canada</th>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chile</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>China</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Croatia</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Denmark</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Germany</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>India</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ireland</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mexico</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Netherlands</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Zealand</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Norway</th>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philippines</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portugal</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spain</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sweden</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK</th>\n",
       "      <td>179</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USA</th>\n",
       "      <td>347</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  Gender  Nationality  file\n",
       "Nationality                                \n",
       "Australia     37       2            1   500\n",
       "Canada        52       2            1   500\n",
       "Chile          3       2            1    91\n",
       "China          2       2            1   188\n",
       "Croatia        3       2            1   144\n",
       "Denmark        3       2            1   141\n",
       "Germany        9       2            1   500\n",
       "India         25       2            1   500\n",
       "Ireland       15       2            1   500\n",
       "Italy          8       2            1   274\n",
       "Mexico         6       2            1   392\n",
       "Netherlands    3       2            1   159\n",
       "New Zealand    8       2            1   500\n",
       "Norway        19       2            1   500\n",
       "Philippines    3       2            1   243\n",
       "Portugal       2       2            1   118\n",
       "Spain          3       2            1    88\n",
       "Sweden         5       2            1   292\n",
       "UK           179       2            1   500\n",
       "USA          347       2            1   500"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.groupby('Nationality').nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unused Section: Leave just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused\n",
    "def extract_features(signal, sr, n_fft, hop_length, n_mfcc):    \n",
    "    # Extract short-time fourier transform\n",
    "    stft = librosa.core.stft(signal, hop_length=hop_length, n_fft=n_fft)\n",
    "    \n",
    "    # Extract log spectrogram\n",
    "    log_spectrogram = extract_spectrogram(stft)\n",
    "    \n",
    "    # Extract MFCC\n",
    "    MFCC = librosa.feature.mfcc(signal, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc)\n",
    "    \n",
    "    # Extract chromagram\n",
    "    chromagram = librosa.feature.chroma_stft(signal, sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    # Extract harmonics and percussion\n",
    "    harmonics, percussion = extract_harmonics_percussion(stft)\n",
    "    \n",
    "    return log_spectrogram, MFCC, chromagram, harmonics, percussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spectrogram(stft):\n",
    "    spectrogram = np.abs(stft)\n",
    "    log_spectrogram = librosa.amplitude_to_db(spectrogram) #amplitude as a function of time and frequency\n",
    "    \n",
    "    return log_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_harmonics_percussion(stft):\n",
    "    harm, perc = librosa.decompose.hpss(stft)\n",
    "    harm = librosa.amplitude_to_db(np.abs(harm))\n",
    "    perc = librosa.amplitude_to_db(np.abs(perc))\n",
    "    \n",
    "    return harm, perc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter frequency using FFT\n",
    "def filter_signal(signal):\n",
    "    # Take the Fourier transform of the data\n",
    "    F = np.fft.fft(signal)\n",
    "\n",
    "    # Filter out any with magnitude < 20\n",
    "    F_filtered = np.array([0.0 if np.abs(x) < 20 else x for x in F])\n",
    "\n",
    "    # Reconstruct the filtered signal\n",
    "    filtered_signal = np.fft.ifft(F_filtered)\n",
    "    filtered_signal = np.array([float(x) for x in filtered_signal])\n",
    "    \n",
    "    return filtered_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(file, sr, n_fft, hop_length):\n",
    "    # Get signal from file\n",
    "    signal, sampling_rate = librosa.load(file, sr=sr, duration=3)\n",
    "    \n",
    "    # Filter out noise\n",
    "    filt_signal = filter_signal(signal)\n",
    "    \n",
    "    # Extract short-time fourier transform\n",
    "    stft = librosa.core.stft(filt_signal, hop_length=hop_length, n_fft=n_fft)\n",
    "    \n",
    "    # Extract log spectrogram\n",
    "    log_spectrogram = extract_spectrogram(stft)\n",
    "    \n",
    "    return log_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MFCC(file, n_mfcc):\n",
    "    # Get signal from file\n",
    "    signal, sampling_rate = librosa.load(file)\n",
    "    \n",
    "    # Filter out noise\n",
    "    filt_signal = filter_signal(signal)\n",
    "    \n",
    "    # Extract MFCC\n",
    "    MFCC = librosa.feature.mfcc(filt_signal, sr=sampling_rate, n_mfcc=n_mfcc)\n",
    "#     MFCC_processed = np.mean(MFCC.T,axis=0)\n",
    "    \n",
    "    return MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MFCC_old(file, n_fft, hop_length, n_mfcc):\n",
    "    # Get signal from file\n",
    "    signal, sampling_rate = librosa.load(file)\n",
    "    \n",
    "    # Filter out noise\n",
    "    filt_signal = filter_signal(signal)\n",
    "    \n",
    "    # Extract MFCC\n",
    "    MFCC = librosa.feature.mfcc(filt_signal, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc)\n",
    "    MFCC_processed = np.mean(MFCC.T,axis=0)\n",
    "    \n",
    "    return MFCC_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chromagram(file, sr, hop_length):\n",
    "    # Get signal from file\n",
    "    signal, sampling_rate = librosa.load(file, sr=sr, duration=3)\n",
    "    \n",
    "    # Filter out noise\n",
    "    filt_signal = filter_signal(signal)\n",
    "    \n",
    "    # Extract chromagram\n",
    "    chromagram = librosa.feature.chroma_stft(filt_signal, sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    return chromagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_harmonics_percussion(signal):    \n",
    "    # Get signal from file\n",
    "    signal, sampling_rate = librosa.load(file)\n",
    "    \n",
    "    # Filter out noise\n",
    "    filt_signal = filter_signal(signal)\n",
    "    \n",
    "    # Extract short-time fourier transform\n",
    "    stft = librosa.core.stft(filt_signal)\n",
    "    \n",
    "    # Extract harmonics and percussion\n",
    "    harmonics, percussion = librosa.decompose.hpss(stft)\n",
    "    harmonics = librosa.amplitude_to_db(np.abs(harmonics))\n",
    "    percussion = librosa.amplitude_to_db(np.abs(percussion))\n",
    "    \n",
    "    return harmonics, percussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction: Actual Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "sr=8000                  # sampling rate\n",
    "n_fft=2048               # number of samples\n",
    "hop_length=512           # amount we shift each fourier transfer to the right\n",
    "n_mfcc=13                # number of MFCCs to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain labels (as strings)\n",
    "labels = np.array(samples['Nationality'])\n",
    "gender = np.array(samples['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract spectrograms to start\n",
    "features_spectrogram = samples['file'].apply(lambda x: get_spectrogram(x,sr,n_fft,hop_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MFCCs\n",
    "features_MFCC = samples['file'].apply(lambda x: get_MFCC(x,n_mfcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract chromagrams\n",
    "features_chromagram = samples['file'].apply(lambda x: get_chromagram(x,sr,hop_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-0e00b1ce7fcd>:11: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  filtered_signal = np.array([float(x) for x in filtered_signal])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-fcd1eeced8e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Extract spectrograms to start\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfeatures_harm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_perc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'file'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mget_harmonics_percussion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Extract harmonics and percussion\n",
    "features_harm, features_perc = samples['file'].apply(lambda x: get_harmonics_percussion(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast spectrogram series into array\n",
    "arr_features_spectrogram = np.array(features_spectrogram)\n",
    "\n",
    "# Save spectrograms\n",
    "np.save('features_spectrogram.npy', arr_features_spectrogram)\n",
    "np.save('labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast MFCCs series into array\n",
    "arr_features_MFCCs = np.array(features_MFCC)\n",
    "\n",
    "# Save MFCCs\n",
    "# np.save('features_MFCC.npy', arr_features_MFCCs)\n",
    "# np.save('labels.npy', labels)\n",
    "\n",
    "np.save('features_MFCC_13_unflattened.npy', arr_features_MFCCs)\n",
    "np.save('labels_40.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast chromagram series into array\n",
    "arr_features_chromagram = np.array(features_chromagram)\n",
    "\n",
    "# Save MFCCs\n",
    "np.save('features_chromagram.npy', arr_features_chromagram)\n",
    "np.save('labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_harm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-86291f4dfb5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Cast harmonics and percussion series into array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0marr_features_harm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_harm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0marr_features_perc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_perc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Save MFCCs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features_harm' is not defined"
     ]
    }
   ],
   "source": [
    "# Cast harmonics and percussion series into array\n",
    "arr_features_harm = np.array(features_harm)\n",
    "arr_features_perc = np.array(features_perc)\n",
    "\n",
    "# Save MFCCs\n",
    "np.save('features_harmonics.npy', arr_features_harm)\n",
    "np.save('features_percussion.npy', arr_features_perc)\n",
    "np.save('labels.npy', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Manipulate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload features when needed\n",
    "# features=np.load('features_spectrogram.npy',allow_pickle=True)\n",
    "features_MFCC=np.load('features_MFCC_40.npy',allow_pickle=True)\n",
    "features_chromagram=np.load('features_chromagram.npy',allow_pickle=True)\n",
    "labels_str=np.load('labels_40.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten features\n",
    "features_MFCC_flattened = []\n",
    "features_chromagram_flattened = []\n",
    "for i in range(len(features_MFCC)):\n",
    "    features_MFCC_flattened.append(features_MFCC[i].flatten())\n",
    "    features_chromagram_flattened.append(features_chromagram[i].flatten())\n",
    "\n",
    "features_MFCC_flattened = np.array(features_MFCC_flattened)\n",
    "features_chromagram_flattened = np.array(features_chromagram_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features_MFCC_flattened=scaler.fit_transform(features_MFCC_flattened)\n",
    "\n",
    "scaler1 = StandardScaler()\n",
    "features_chromagram_flattened=scaler1.fit_transform(features_chromagram_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features\n",
    "features = []\n",
    "for i in range(len(features_MFCC_flattened)):\n",
    "    combo = np.append(features_chromagram_flattened[i],features_MFCC_flattened[i])\n",
    "    features.append(combo)\n",
    "    \n",
    "features=np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn labels into numeric values, create dictionary to map back later \n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels_str)\n",
    "labels=le.transform(labels_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast array to type float64 instead of object for tensors to work\n",
    "features = [np.array(list(x),dtype=np.float64) for x in features]\n",
    "features = np.array(features,dtype=np.float64)\n",
    "\n",
    "labels = [np.long(x) for x in labels]\n",
    "labels = np.array(labels,dtype=np.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train and test sets: NORMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=142)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train and test sets: ZERO-SHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_male = []\n",
    "labels_male = []\n",
    "features_female = []\n",
    "labels_female = []\n",
    "\n",
    "for i in range(len(gender)):\n",
    "    if gender[i] == 'm':\n",
    "        features_male.append(features[i])\n",
    "        labels_male.append(labels[i])\n",
    "    else:\n",
    "        features_female.append(features[i])\n",
    "        labels_female.append(labels[i])\n",
    "        \n",
    "features_male = np.array(features_male)\n",
    "features_female = np.array(features_female)\n",
    "labels_male = np.array(labels_male)\n",
    "labels_female = np.array(labels_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 8, 8, ..., 5, 5, 5])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2672"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test datasets\n",
    "X_train, X_val, y_train, y_val = train_test_split(features_male, labels_male, test_size=0.3, random_state=42)\n",
    "_, X_test, _, y_test = train_test_split(features_female, labels_female, test_size=0.3, random_state=142)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_and_accuracy(network, data_loader):\n",
    "    total_loss = 0\n",
    "    num_correct = 0\n",
    "    num_instances = 0\n",
    "\n",
    "    cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for X, y in data_loader:\n",
    "        with torch.no_grad():\n",
    "            y_pred = network(X)\n",
    "            total_loss += cross_entropy_loss(y_pred,y).item() * X.size(0)\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            predicted = torch.argmax(y_pred[i])\n",
    "            actual = y[i]\n",
    "\n",
    "            if predicted == actual:\n",
    "                num_correct += 1\n",
    "                \n",
    "        num_instances += X.size(0)\n",
    "  \n",
    "    accuracy = num_correct / num_instances\n",
    "    average_loss = total_loss / num_instances\n",
    "\n",
    "    return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(network, train_data_loader, valid_data_loader, test_data_loader, optimizer):\n",
    "    train_losses = []\n",
    "    valid_accs = []\n",
    "\n",
    "    cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(100):\n",
    "        print('Epoch: ' + str(epoch))\n",
    "        total_loss = 0.0\n",
    "        num_instances = 0\n",
    "\n",
    "        for X, y in train_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = network(X)\n",
    "\n",
    "            loss = cross_entropy_loss(y_pred,y)\n",
    "            total_loss+=loss.item() * X.size(0)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            num_instances += X.size(0)\n",
    "\n",
    "        train_loss = total_loss / num_instances\n",
    "        train_acc, _ = compute_loss_and_accuracy(network, train_data_loader)\n",
    "        valid_acc, _ = compute_loss_and_accuracy(network, valid_data_loader)\n",
    "        print(\"Train accuracy: \",train_acc)\n",
    "        print(\"Valid accuracy: \",valid_acc)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        valid_accs.append(valid_acc)\n",
    "    test_acc, _ = compute_loss_and_accuracy(network, test_data_loader)\n",
    "    print(\"Test accuracy: \",test_acc)\n",
    "    \n",
    "    return train_losses, valid_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MFCC Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network=nn.Sequential(\n",
    "            nn.Linear(604,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 20),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        return self.network(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tensors - Sequential\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=64,shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=64)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train accuracy:  0.07978339350180505\n",
      "Valid accuracy:  0.08501683501683502\n",
      "Epoch: 1\n",
      "Train accuracy:  0.05812274368231047\n",
      "Valid accuracy:  0.06060606060606061\n",
      "Epoch: 2\n",
      "Train accuracy:  0.11263537906137185\n",
      "Valid accuracy:  0.10101010101010101\n",
      "Epoch: 3\n",
      "Train accuracy:  0.14693140794223827\n",
      "Valid accuracy:  0.12205387205387205\n",
      "Epoch: 4\n",
      "Train accuracy:  0.1523465703971119\n",
      "Valid accuracy:  0.12457912457912458\n",
      "Epoch: 5\n",
      "Train accuracy:  0.1548736462093863\n",
      "Valid accuracy:  0.13131313131313133\n",
      "Epoch: 6\n",
      "Train accuracy:  0.16462093862815885\n",
      "Valid accuracy:  0.12794612794612795\n",
      "Epoch: 7\n",
      "Train accuracy:  0.17833935018050542\n",
      "Valid accuracy:  0.15572390572390574\n",
      "Epoch: 8\n",
      "Train accuracy:  0.17942238267148014\n",
      "Valid accuracy:  0.15151515151515152\n",
      "Epoch: 9\n",
      "Train accuracy:  0.1888086642599278\n",
      "Valid accuracy:  0.15993265993265993\n",
      "Epoch: 10\n",
      "Train accuracy:  0.19422382671480146\n",
      "Valid accuracy:  0.1590909090909091\n",
      "Epoch: 11\n",
      "Train accuracy:  0.20541516245487365\n",
      "Valid accuracy:  0.1675084175084175\n",
      "Epoch: 12\n",
      "Train accuracy:  0.21660649819494585\n",
      "Valid accuracy:  0.1835016835016835\n",
      "Epoch: 13\n",
      "Train accuracy:  0.21516245487364621\n",
      "Valid accuracy:  0.18265993265993266\n",
      "Epoch: 14\n",
      "Train accuracy:  0.2176895306859206\n",
      "Valid accuracy:  0.18013468013468015\n",
      "Epoch: 15\n",
      "Train accuracy:  0.2263537906137184\n",
      "Valid accuracy:  0.1893939393939394\n",
      "Epoch: 16\n",
      "Train accuracy:  0.2111913357400722\n",
      "Valid accuracy:  0.1776094276094276\n",
      "Epoch: 17\n",
      "Train accuracy:  0.23140794223826716\n",
      "Valid accuracy:  0.1936026936026936\n",
      "Epoch: 18\n",
      "Train accuracy:  0.23176895306859205\n",
      "Valid accuracy:  0.19865319865319866\n",
      "Epoch: 19\n",
      "Train accuracy:  0.23574007220216606\n",
      "Valid accuracy:  0.20286195286195285\n",
      "Epoch: 20\n",
      "Train accuracy:  0.244043321299639\n",
      "Valid accuracy:  0.20286195286195285\n",
      "Epoch: 21\n",
      "Train accuracy:  0.2444043321299639\n",
      "Valid accuracy:  0.20454545454545456\n",
      "Epoch: 22\n",
      "Train accuracy:  0.2592057761732852\n",
      "Valid accuracy:  0.2079124579124579\n",
      "Epoch: 23\n",
      "Train accuracy:  0.2602888086642599\n",
      "Valid accuracy:  0.20875420875420875\n",
      "Epoch: 24\n",
      "Train accuracy:  0.2631768953068592\n",
      "Valid accuracy:  0.20707070707070707\n",
      "Epoch: 25\n",
      "Train accuracy:  0.2682310469314079\n",
      "Valid accuracy:  0.20622895622895623\n",
      "Epoch: 26\n",
      "Train accuracy:  0.27870036101083034\n",
      "Valid accuracy:  0.20959595959595959\n",
      "Epoch: 27\n",
      "Train accuracy:  0.2779783393501805\n",
      "Valid accuracy:  0.20454545454545456\n",
      "Epoch: 28\n",
      "Train accuracy:  0.2776173285198556\n",
      "Valid accuracy:  0.2079124579124579\n",
      "Epoch: 29\n",
      "Train accuracy:  0.2819494584837545\n",
      "Valid accuracy:  0.21296296296296297\n",
      "Epoch: 30\n",
      "Train accuracy:  0.28231046931407944\n",
      "Valid accuracy:  0.22138047138047137\n",
      "Epoch: 31\n",
      "Train accuracy:  0.2819494584837545\n",
      "Valid accuracy:  0.21043771043771045\n",
      "Epoch: 32\n",
      "Train accuracy:  0.296028880866426\n",
      "Valid accuracy:  0.21212121212121213\n",
      "Epoch: 33\n",
      "Train accuracy:  0.3061371841155235\n",
      "Valid accuracy:  0.22474747474747475\n",
      "Epoch: 34\n",
      "Train accuracy:  0.30144404332129965\n",
      "Valid accuracy:  0.21801346801346802\n",
      "Epoch: 35\n",
      "Train accuracy:  0.30288808664259925\n",
      "Valid accuracy:  0.2138047138047138\n",
      "Epoch: 36\n",
      "Train accuracy:  0.3144404332129964\n",
      "Valid accuracy:  0.22474747474747475\n",
      "Epoch: 37\n",
      "Train accuracy:  0.3072202166064982\n",
      "Valid accuracy:  0.21632996632996632\n",
      "Epoch: 38\n",
      "Train accuracy:  0.31660649819494585\n",
      "Valid accuracy:  0.22474747474747475\n",
      "Epoch: 39\n",
      "Train accuracy:  0.32418772563176895\n",
      "Valid accuracy:  0.22727272727272727\n",
      "Epoch: 40\n",
      "Train accuracy:  0.3306859205776173\n",
      "Valid accuracy:  0.22053872053872053\n",
      "Epoch: 41\n",
      "Train accuracy:  0.3418772563176895\n",
      "Valid accuracy:  0.22053872053872053\n",
      "Epoch: 42\n",
      "Train accuracy:  0.33754512635379064\n",
      "Valid accuracy:  0.2297979797979798\n",
      "Epoch: 43\n",
      "Train accuracy:  0.344043321299639\n",
      "Valid accuracy:  0.23316498316498316\n",
      "Epoch: 44\n",
      "Train accuracy:  0.34657039711191334\n",
      "Valid accuracy:  0.22053872053872053\n",
      "Epoch: 45\n",
      "Train accuracy:  0.34873646209386283\n",
      "Valid accuracy:  0.2297979797979798\n",
      "Epoch: 46\n",
      "Train accuracy:  0.35415162454873644\n",
      "Valid accuracy:  0.2297979797979798\n",
      "Epoch: 47\n",
      "Train accuracy:  0.3649819494584838\n",
      "Valid accuracy:  0.23316498316498316\n",
      "Epoch: 48\n",
      "Train accuracy:  0.35703971119133576\n",
      "Valid accuracy:  0.22643097643097643\n",
      "Epoch: 49\n",
      "Train accuracy:  0.36606498194945847\n",
      "Valid accuracy:  0.2297979797979798\n",
      "Epoch: 50\n",
      "Train accuracy:  0.3675090252707581\n",
      "Valid accuracy:  0.24242424242424243\n",
      "Epoch: 51\n",
      "Train accuracy:  0.36570397111913355\n",
      "Valid accuracy:  0.2255892255892256\n",
      "Epoch: 52\n",
      "Train accuracy:  0.3725631768953069\n",
      "Valid accuracy:  0.21801346801346802\n",
      "Epoch: 53\n",
      "Train accuracy:  0.3815884476534296\n",
      "Valid accuracy:  0.23653198653198654\n",
      "Epoch: 54\n",
      "Train accuracy:  0.3844765342960289\n",
      "Valid accuracy:  0.23148148148148148\n",
      "Epoch: 55\n",
      "Train accuracy:  0.3953068592057762\n",
      "Valid accuracy:  0.2382154882154882\n",
      "Epoch: 56\n",
      "Train accuracy:  0.3819494584837545\n",
      "Valid accuracy:  0.23316498316498316\n",
      "Epoch: 57\n",
      "Train accuracy:  0.3898916967509025\n",
      "Valid accuracy:  0.23905723905723905\n",
      "Epoch: 58\n",
      "Train accuracy:  0.4075812274368231\n",
      "Valid accuracy:  0.23653198653198654\n",
      "Epoch: 59\n",
      "Train accuracy:  0.4086642599277978\n",
      "Valid accuracy:  0.23232323232323232\n",
      "Epoch: 60\n",
      "Train accuracy:  0.4028880866425993\n",
      "Valid accuracy:  0.23148148148148148\n",
      "Epoch: 61\n",
      "Train accuracy:  0.41696750902527074\n",
      "Valid accuracy:  0.23316498316498316\n",
      "Epoch: 62\n",
      "Train accuracy:  0.4180505415162455\n",
      "Valid accuracy:  0.23316498316498316\n",
      "Epoch: 63\n",
      "Train accuracy:  0.41335740072202165\n",
      "Valid accuracy:  0.22727272727272727\n",
      "Epoch: 64\n",
      "Train accuracy:  0.41696750902527074\n",
      "Valid accuracy:  0.23063973063973064\n",
      "Epoch: 65\n",
      "Train accuracy:  0.4328519855595668\n",
      "Valid accuracy:  0.23148148148148148\n",
      "Epoch: 66\n",
      "Train accuracy:  0.42418772563176893\n",
      "Valid accuracy:  0.23905723905723905\n",
      "Epoch: 67\n",
      "Train accuracy:  0.43646209386281587\n",
      "Valid accuracy:  0.23316498316498316\n",
      "Epoch: 68\n",
      "Train accuracy:  0.4314079422382672\n",
      "Valid accuracy:  0.2297979797979798\n",
      "Epoch: 69\n",
      "Train accuracy:  0.4299638989169675\n",
      "Valid accuracy:  0.2356902356902357\n",
      "Epoch: 70\n",
      "Train accuracy:  0.43212996389891695\n",
      "Valid accuracy:  0.2281144781144781\n",
      "Epoch: 71\n",
      "Train accuracy:  0.4444043321299639\n",
      "Valid accuracy:  0.22643097643097643\n",
      "Epoch: 72\n",
      "Train accuracy:  0.4393501805054152\n",
      "Valid accuracy:  0.24494949494949494\n",
      "Epoch: 73\n",
      "Train accuracy:  0.45018050541516247\n",
      "Valid accuracy:  0.23232323232323232\n",
      "Epoch: 74\n",
      "Train accuracy:  0.46064981949458483\n",
      "Valid accuracy:  0.2382154882154882\n",
      "Epoch: 75\n",
      "Train accuracy:  0.45379061371841156\n",
      "Valid accuracy:  0.2382154882154882\n",
      "Epoch: 76\n",
      "Train accuracy:  0.463898916967509\n",
      "Valid accuracy:  0.23653198653198654\n",
      "Epoch: 77\n",
      "Train accuracy:  0.455956678700361\n",
      "Valid accuracy:  0.2415824915824916\n",
      "Epoch: 78\n",
      "Train accuracy:  0.46859205776173285\n",
      "Valid accuracy:  0.24242424242424243\n",
      "Epoch: 79\n",
      "Train accuracy:  0.4707581227436823\n",
      "Valid accuracy:  0.22727272727272727\n",
      "Epoch: 80\n",
      "Train accuracy:  0.46462093862815884\n",
      "Valid accuracy:  0.2382154882154882\n",
      "Epoch: 81\n",
      "Train accuracy:  0.4754512635379061\n",
      "Valid accuracy:  0.2441077441077441\n",
      "Epoch: 82\n",
      "Train accuracy:  0.4790613718411552\n",
      "Valid accuracy:  0.234006734006734\n",
      "Epoch: 83\n",
      "Train accuracy:  0.4703971119133574\n",
      "Valid accuracy:  0.2297979797979798\n",
      "Epoch: 84\n",
      "Train accuracy:  0.4815884476534296\n",
      "Valid accuracy:  0.23905723905723905\n",
      "Epoch: 85\n",
      "Train accuracy:  0.49169675090252707\n",
      "Valid accuracy:  0.234006734006734\n",
      "Epoch: 86\n",
      "Train accuracy:  0.48953068592057764\n",
      "Valid accuracy:  0.23232323232323232\n",
      "Epoch: 87\n",
      "Train accuracy:  0.4927797833935018\n",
      "Valid accuracy:  0.24242424242424243\n",
      "Epoch: 88\n",
      "Train accuracy:  0.5061371841155234\n",
      "Valid accuracy:  0.2356902356902357\n",
      "Epoch: 89\n",
      "Train accuracy:  0.49927797833935017\n",
      "Valid accuracy:  0.2382154882154882\n",
      "Epoch: 90\n",
      "Train accuracy:  0.5093862815884477\n",
      "Valid accuracy:  0.234006734006734\n",
      "Epoch: 91\n"
     ]
    }
   ],
   "source": [
    "network_mfcc = Sequential()\n",
    "sgd = torch.optim.Adam(network_mfcc.parameters(), lr=0.0001)\n",
    "\n",
    "train_losses, valid_accs = run_experiment(network_mfcc, train_data_loader, valid_data_loader, test_data_loader, sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectogram: IGNORE UNUSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1,out_channels=10,kernel_size=2,stride=1,padding=0)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=10, out_channels=32, kernel_size=2, stride=1, padding=0)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(in_features=32*2*11,out_features=100)\n",
    "        self.drop1 = torch.nn.Dropout(0.2)\n",
    "        self.fc2 = torch.nn.Linear(in_features=100,out_features=50)\n",
    "        self.drop2 = torch.nn.Dropout(0.2)\n",
    "        self.fc3 = torch.nn.Linear(in_features=50,out_features=20)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size = 64\n",
    "        X = self.conv1(X)\n",
    "        X = self.pool1(X)\n",
    "        X = self.conv2(X)\n",
    "        X = self.pool2(X)\n",
    "        X = X.relu()\n",
    "        X = X.view(batch_size, -1)\n",
    "        X = self.fc1(X)\n",
    "        X - self.drop1(X)\n",
    "        X = X.relu()\n",
    "        X = self.fc2(X)\n",
    "        X - self.drop2(X)\n",
    "        X = X.relu()\n",
    "        X = self.fc3(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Sequential(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.network=nn.Sequential(\n",
    "#             nn.Linear(40,128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(128, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(256, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(512,64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(64,20),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "#     def forward(self, X):\n",
    "#         return self.network(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float)\n",
    "X_train_tensor = X_train_tensor.reshape([X_train.shape[0],1,X_train.shape[1],X_train.shape[2]])\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float)\n",
    "X_test_tensor = X_test_tensor.reshape([X_test.shape[0],1,X_test.shape[1],X_test.shape[2]])\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=64,shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Convolutional()\n",
    "sgd = torch.optim.SGD(network.parameters(), lr=0.001)\n",
    "\n",
    "train_losses, valid_accs = run_experiment(network, train_data_loader, valid_data_loader, sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly drop some to make batch sizes even\n",
    "# drop = random.sample(range(0,X_train.shape[0]),4608)\n",
    "# X_train=X_train[list(drop)]\n",
    "# y_train=y_train[list(drop)]\n",
    "\n",
    "# drop = random.sample(range(0,X_test.shape[0]),1984)\n",
    "# X_test=X_test[list(drop)]\n",
    "# y_test=y_test[list(drop)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
