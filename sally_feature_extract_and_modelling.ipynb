{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa, librosa.display\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.transforms import transforms\n",
    "from sklearn.preprocessing import StandardScaler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Relevant Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(r'C:\\Users\\sally\\Documents\\Fall 2020\\CIS 519 - Intro to Machine Learning\\Project\\audioclassification_meta.csv')\n",
    "# meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r'C:\\Users\\sally\\Documents\\vox1_dev_wav\\wav'\n",
    "filenames = []\n",
    "for foldername in os.listdir(directory):\n",
    "    folder_dir = os.path.join(directory,foldername)\n",
    "    for subfoldername in os.listdir(folder_dir):\n",
    "        subfolder_dir = os.path.join(folder_dir,subfoldername)\n",
    "        for filename in os.listdir(subfolder_dir):\n",
    "            file = os.path.join(subfolder_dir,filename)\n",
    "            filenames.append((foldername,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = pd.DataFrame(filenames)\n",
    "files.rename(columns={0:'ID',1:'file'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_df = meta.merge(files,left_on='VoxCeleb1 ID',right_on='ID')[['ID','Gender','Nationality','file']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain list of nationalities and count of gender\n",
    "nationality_gender_count=pd.DataFrame(wav_df.groupby('Nationality')['Gender'].nunique()).reset_index()\n",
    "nationality_gender_count=nationality_gender_count[nationality_gender_count['Gender'] > 1]['Nationality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only nationalities with samples from both genders\n",
    "wav_df=wav_df.merge(nationality_gender_count,left_on='Nationality',right_on='Nationality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of nationalities\n",
    "listNat=wav_df['Nationality'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cap samples at 500 max\n",
    "columns=['ID', 'Gender', 'Nationality','file']\n",
    "samples = pd.DataFrame(columns=columns)\n",
    "for i in range(len(listNat)):\n",
    "    total = wav_df[wav_df['Nationality'] == listNat[i]].count()['ID']\n",
    "    num = int(0.5 * total)\n",
    "    if num < 500:\n",
    "        samp = wav_df[wav_df['Nationality'] == listNat[i]].sample(num,random_state=42)\n",
    "    else:\n",
    "        samp = wav_df[wav_df['Nationality'] == listNat[i]].sample(500,random_state=42)\n",
    "    samples = samples.append(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nationality</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Australia</th>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Canada</th>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chile</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>China</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Croatia</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Denmark</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Germany</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>India</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ireland</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mexico</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Netherlands</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Zealand</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Norway</th>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philippines</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portugal</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spain</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sweden</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK</th>\n",
       "      <td>179</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USA</th>\n",
       "      <td>347</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  Gender  Nationality  file\n",
       "Nationality                                \n",
       "Australia     37       2            1   500\n",
       "Canada        52       2            1   500\n",
       "Chile          3       2            1    91\n",
       "China          2       2            1   188\n",
       "Croatia        3       2            1   144\n",
       "Denmark        3       2            1   141\n",
       "Germany        9       2            1   500\n",
       "India         25       2            1   500\n",
       "Ireland       15       2            1   500\n",
       "Italy          8       2            1   274\n",
       "Mexico         6       2            1   392\n",
       "Netherlands    3       2            1   159\n",
       "New Zealand    8       2            1   500\n",
       "Norway        19       2            1   500\n",
       "Philippines    3       2            1   243\n",
       "Portugal       2       2            1   118\n",
       "Spain          3       2            1    88\n",
       "Sweden         5       2            1   292\n",
       "UK           179       2            1   500\n",
       "USA          347       2            1   500"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.groupby('Nationality').nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter frequency using FFT\n",
    "def filter_signal(signal):\n",
    "    # Take the Fourier transform of the data\n",
    "    F = np.fft.fft(signal)\n",
    "\n",
    "    # Filter out any with magnitude < 20\n",
    "    F_filtered = np.array([0.0 if np.abs(x) < 20 else x for x in F])\n",
    "\n",
    "    # Reconstruct the filtered signal\n",
    "    filtered_signal = np.fft.ifft(F_filtered)\n",
    "    filtered_signal = np.array([float(x) for x in filtered_signal])\n",
    "    \n",
    "    return filtered_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(file, sr, n_fft, hop_length):\n",
    "    # Get signal from file\n",
    "    signal, sampling_rate = librosa.load(file, sr=sr, duration=3)\n",
    "    \n",
    "    # Filter out noise\n",
    "    filt_signal = filter_signal(signal)\n",
    "    \n",
    "    # Extract short-time fourier transform\n",
    "    stft = librosa.core.stft(filt_signal, hop_length=hop_length, n_fft=n_fft)\n",
    "    \n",
    "    # Extract log spectrogram\n",
    "    log_spectrogram = extract_spectrogram(stft)\n",
    "    \n",
    "    return log_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MFCC(file, n_mfcc):\n",
    "    # Get signal from file\n",
    "    signal, sampling_rate = librosa.load(file, duration=3)\n",
    "    \n",
    "    # Filter out noise\n",
    "    filt_signal = filter_signal(signal)\n",
    "    \n",
    "    # Extract MFCC\n",
    "    MFCC = librosa.feature.mfcc(filt_signal, sr=sampling_rate, n_mfcc=n_mfcc)\n",
    "#     MFCC_processed = np.mean(MFCC.T,axis=0)\n",
    "    \n",
    "    return MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chromagram(file, sr, hop_length):\n",
    "    # Get signal from file\n",
    "    signal, sampling_rate = librosa.load(file, duration=3)\n",
    "    \n",
    "    # Filter out noise\n",
    "    filt_signal = filter_signal(signal)\n",
    "    \n",
    "    # Extract chromagram\n",
    "    chromagram = librosa.feature.chroma_stft(filt_signal, sr=sampling_rate, hop_length=hop_length)\n",
    "    \n",
    "    return chromagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_harmonics_percussion(signal):    \n",
    "    # Get signal from file\n",
    "    signal, sampling_rate = librosa.load(file,duration=3)\n",
    "    \n",
    "    # Filter out noise\n",
    "    filt_signal = filter_signal(signal)\n",
    "    \n",
    "    # Extract short-time fourier transform\n",
    "    stft = librosa.core.stft(filt_signal)\n",
    "    \n",
    "    # Extract harmonics and percussion\n",
    "    harmonics, percussion = librosa.decompose.hpss(stft)\n",
    "    harmonics = librosa.amplitude_to_db(np.abs(harmonics))\n",
    "    percussion = librosa.amplitude_to_db(np.abs(percussion))\n",
    "    \n",
    "    return (harmonics, percussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectral_contrast(signal):    \n",
    "    # Get signal from file\n",
    "    signal, sampling_rate = librosa.load(file,duration=3)\n",
    "    \n",
    "    # Filter out noise\n",
    "    filt_signal = filter_signal(signal)\n",
    "    \n",
    "    # Extract short-time fourier transform\n",
    "    stft = librosa.core.stft(filt_signal)\n",
    "    \n",
    "    # Extract harmonics and percussion\n",
    "    spectral = librosa.feature.spectral_contrast(S=stft,sr=sampling_rate)\n",
    "    spectral_processed = np.mean(spectral.T,axis=0)\n",
    "    \n",
    "    return spectral_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction: Actual Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "sr=8000                  # sampling rate\n",
    "n_fft=2048               # number of samples\n",
    "hop_length=512           # amount we shift each fourier transfer to the right\n",
    "n_mfcc=13                # number of MFCCs to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain labels (as strings)\n",
    "labels = np.array(samples['Nationality'])\n",
    "gender = np.array(samples['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract spectrograms to start\n",
    "features_spectrogram = samples['file'].apply(lambda x: get_spectrogram(x,sr,n_fft,hop_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MFCCs\n",
    "features_MFCC = samples['file'].apply(lambda x: get_MFCC(x,n_mfcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract chromagrams\n",
    "features_chromagram = samples['file'].apply(lambda x: get_chromagram(x,sr,hop_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract harmonics and percussion\n",
    "features_harm_perc = samples['file'].apply(lambda x: get_harmonics_percussion(x))\n",
    "\n",
    "features_harm_perc=np.array(features_harm_perc)\n",
    "\n",
    "harmonics=[]\n",
    "percussion=[]\n",
    "for i in range(len(features_harm_perc)):\n",
    "    harmonics.append(features_harm_perc[i][0])\n",
    "    percussion.append(features_harm_perc[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-0e00b1ce7fcd>:11: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  filtered_signal = np.array([float(x) for x in filtered_signal])\n",
      "C:\\Users\\sally\\anaconda3\\lib\\site-packages\\librosa\\feature\\spectral.py:554: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  valley[k] = np.mean(sortedr[:idx], axis=0)\n",
      "C:\\Users\\sally\\anaconda3\\lib\\site-packages\\librosa\\feature\\spectral.py:555: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  peak[k] = np.mean(sortedr[-idx:], axis=0)\n"
     ]
    }
   ],
   "source": [
    "# Extract spectral contrast\n",
    "features_spectral = samples['file'].apply(lambda x: get_spectral_contrast(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "727       [100.75601799963864, 106.26759386974658, 107.6...\n",
       "907       [100.75601799963864, 106.26759386974658, 107.6...\n",
       "1547      [100.75601799963864, 106.26759386974658, 107.6...\n",
       "240       [100.75601799963864, 106.26759386974658, 107.6...\n",
       "686       [100.75601799963864, 106.26759386974658, 107.6...\n",
       "                                ...                        \n",
       "145646    [100.75601799963864, 106.26759386974658, 107.6...\n",
       "145559    [100.75601799963864, 106.26759386974658, 107.6...\n",
       "145669    [100.75601799963864, 106.26759386974658, 107.6...\n",
       "145486    [100.75601799963864, 106.26759386974658, 107.6...\n",
       "145509    [100.75601799963864, 106.26759386974658, 107.6...\n",
       "Name: file, Length: 6630, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfeatures_spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast spectrogram series into array\n",
    "arr_features_spectrogram = np.array(features_spectrogram)\n",
    "\n",
    "# Save spectrograms\n",
    "np.save('features_spectrogram.npy', arr_features_spectrogram)\n",
    "np.save('labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast MFCCs series into array\n",
    "arr_features_MFCCs = np.array(features_MFCC)\n",
    "\n",
    "# Save MFCCs\n",
    "# np.save('features_MFCC.npy', arr_features_MFCCs)\n",
    "# np.save('labels.npy', labels)\n",
    "\n",
    "np.save('features_MFCC_13_full.npy', arr_features_MFCCs)\n",
    "np.save('labels_13.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast chromagram series into array\n",
    "arr_features_chromagram = np.array(features_chromagram)\n",
    "\n",
    "# Save MFCCs\n",
    "np.save('features_chromagram.npy', arr_features_chromagram)\n",
    "np.save('labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast harmonics and percussion series into array\n",
    "# arr_features_harm = np.array(harmonics)\n",
    "# arr_features_perc = np.array(percussion)\n",
    "\n",
    "# Save MFCCs\n",
    "np.save('features_harmonics.npy', harmonics)\n",
    "np.save('features_percussion.npy', percussion)\n",
    "np.save('labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast spectral contrast series into array\n",
    "arr_features_spectrogram = np.array(features_spectrogram)\n",
    "\n",
    "# Save spectrograms\n",
    "np.save('features_spectrogram.npy', arr_features_spectrogram)\n",
    "np.save('labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = np.array([0 if x=='m' else 1 for x in gender])\n",
    "np.save('gender.npy',gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Manipulate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload features when needed\n",
    "# features=np.load('features_spectrogram.npy',allow_pickle=True)\n",
    "features_MFCC=np.load('features_MFCC_40.npy',allow_pickle=True)\n",
    "features_chromagram=np.load('features_chromagram.npy',allow_pickle=True)\n",
    "features_harmonics=np.load('features_harmonics_mean.npy',allow_pickle=True)\n",
    "features_percussion=np.load('features_percussion_mean.npy',allow_pickle=True)\n",
    "labels_str=np.load('labels.npy',allow_pickle=True)\n",
    "gender=np.load('gender.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# harmonics_mean = []\n",
    "# percussion_mean= []\n",
    "\n",
    "# for i in range(len(harmonics)):\n",
    "#     harmonics_mean.append(np.mean(harmonics[i].T,axis=0))\n",
    "#     percussion_mean.append(np.mean(percussion[i].T,axis=0))\n",
    "    \n",
    "# harmonics_mean = np.array(harmonics_mean)\n",
    "# percussion_mean = np.array(percussion_mean)\n",
    "\n",
    "# np.save('features_harmonics_mean.npy', harmonics_mean)\n",
    "# np.save('features_percussion_mean.npy', percussion_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromagram_mean = []\n",
    "\n",
    "for i in range(len(features_chromagram)):\n",
    "    chromagram_mean.append(np.mean(features_chromagram[i].T,axis=0))\n",
    "    \n",
    "chromagram_mean = np.array(chromagram_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten features\n",
    "features_MFCC_flattened = []\n",
    "features_chromagram_flattened = []\n",
    "for i in range(len(features_MFCC)):\n",
    "    features_MFCC_flattened.append(features_MFCC[i].flatten())\n",
    "    features_chromagram_flattened.append(features_chromagram[i].flatten())\n",
    "\n",
    "features_MFCC_flattened = np.array(features_MFCC_flattened)\n",
    "features_chromagram_flattened = np.array(features_chromagram_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# features_MFCC_flattened=scaler.fit_transform(features_MFCC_flattened)\n",
    "\n",
    "# scaler1 = StandardScaler()\n",
    "# features_chromagram_flattened=scaler1.fit_transform(features_chromagram_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features\n",
    "features = []\n",
    "for i in range(len(features_MFCC)):\n",
    "    combo = np.append(features_MFCC_flattened[i],features_chromagram_flattened[i])\n",
    "#     combo = np.append(combo,features_harmonics[i])\n",
    "#     combo = np.append(combo,features_percussion[i])\n",
    "    features.append(combo)\n",
    "    \n",
    "features=np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn labels into numeric values, create dictionary to map back later \n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels_str)\n",
    "labels=le.transform(labels_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast array to type float64 instead of object for tensors to work\n",
    "features = [np.array(list(x),dtype=np.float64) for x in features]\n",
    "features = np.array(features,dtype=np.float64)\n",
    "\n",
    "labels = [np.long(x) for x in labels]\n",
    "labels = np.array(labels,dtype=np.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train and test sets: NORMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=142)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train and test sets: ZERO-SHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_male = []\n",
    "labels_male = []\n",
    "features_female = []\n",
    "labels_female = []\n",
    "\n",
    "for i in range(len(gender)):\n",
    "    if gender[i] == 0:\n",
    "        features_male.append(features[i])\n",
    "        labels_male.append(labels[i])\n",
    "    else:\n",
    "        features_female.append(features[i])\n",
    "        labels_female.append(labels[i])\n",
    "        \n",
    "features_male = np.array(features_male)\n",
    "features_female = np.array(features_female)\n",
    "labels_male = np.array(labels_male)\n",
    "labels_female = np.array(labels_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test datasets\n",
    "X_train, X_val, y_train, y_val = train_test_split(features_male, labels_male, test_size=0.3, random_state=42)\n",
    "_, X_test, _, y_test = train_test_split(features_female, labels_female, test_size=0.3, random_state=142)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_and_accuracy(network, data_loader):\n",
    "    total_loss = 0\n",
    "    num_correct = 0\n",
    "    num_instances = 0\n",
    "\n",
    "    cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for X, y in data_loader:\n",
    "        with torch.no_grad():\n",
    "            y_pred = network(X)\n",
    "            total_loss += cross_entropy_loss(y_pred,y).item() * X.size(0)\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            predicted = torch.argmax(y_pred[i])\n",
    "            actual = y[i]\n",
    "\n",
    "            if predicted == actual:\n",
    "                num_correct += 1\n",
    "                \n",
    "        num_instances += X.size(0)\n",
    "  \n",
    "    accuracy = num_correct / num_instances\n",
    "    average_loss = total_loss / num_instances\n",
    "\n",
    "    return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(network, train_data_loader, valid_data_loader, test_data_loader, optimizer):\n",
    "    train_losses = []\n",
    "    valid_accs = []\n",
    "\n",
    "    cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(100):\n",
    "        print('Epoch: ' + str(epoch))\n",
    "        total_loss = 0.0\n",
    "        num_instances = 0\n",
    "\n",
    "        for X, y in train_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = network(X)\n",
    "\n",
    "            loss = cross_entropy_loss(y_pred,y)\n",
    "            total_loss+=loss.item() * X.size(0)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            num_instances += X.size(0)\n",
    "\n",
    "        train_loss = total_loss / num_instances\n",
    "        train_acc, _ = compute_loss_and_accuracy(network, train_data_loader)\n",
    "        valid_acc, _ = compute_loss_and_accuracy(network, valid_data_loader)\n",
    "        print(\"Train accuracy: \",train_acc)\n",
    "        print(\"Valid accuracy: \",valid_acc)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        valid_accs.append(valid_acc)\n",
    "    test_acc, _ = compute_loss_and_accuracy(network, test_data_loader)\n",
    "    print(\"Test accuracy: \",test_acc)\n",
    "    \n",
    "    return train_losses, valid_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MFCC Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network=nn.Sequential(\n",
    "            nn.Linear(604,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 20),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        return self.network(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tensors - Sequential\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=64,shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=64)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train accuracy:  0.10180505415162455\n",
      "Valid accuracy:  0.09595959595959595\n",
      "Epoch: 1\n",
      "Train accuracy:  0.10072202166064982\n",
      "Valid accuracy:  0.08249158249158249\n",
      "Epoch: 2\n",
      "Train accuracy:  0.10072202166064982\n",
      "Valid accuracy:  0.0968013468013468\n",
      "Epoch: 3\n",
      "Train accuracy:  0.11191335740072202\n",
      "Valid accuracy:  0.10016835016835017\n",
      "Epoch: 4\n",
      "Train accuracy:  0.1187725631768953\n",
      "Valid accuracy:  0.11026936026936027\n",
      "Epoch: 5\n",
      "Train accuracy:  0.12707581227436823\n",
      "Valid accuracy:  0.11616161616161616\n",
      "Epoch: 6\n",
      "Train accuracy:  0.13068592057761733\n",
      "Valid accuracy:  0.12037037037037036\n",
      "Epoch: 7\n",
      "Train accuracy:  0.1444043321299639\n",
      "Valid accuracy:  0.117003367003367\n",
      "Epoch: 8\n",
      "Train accuracy:  0.15054151624548737\n",
      "Valid accuracy:  0.12542087542087543\n",
      "Epoch: 9\n",
      "Train accuracy:  0.1523465703971119\n",
      "Valid accuracy:  0.14646464646464646\n",
      "Epoch: 10\n",
      "Train accuracy:  0.16028880866425993\n",
      "Valid accuracy:  0.14562289562289563\n",
      "Epoch: 11\n",
      "Train accuracy:  0.17148014440433212\n",
      "Valid accuracy:  0.16077441077441076\n",
      "Epoch: 12\n",
      "Train accuracy:  0.17870036101083034\n",
      "Valid accuracy:  0.16161616161616163\n",
      "Epoch: 13\n",
      "Train accuracy:  0.18303249097472923\n",
      "Valid accuracy:  0.1632996632996633\n",
      "Epoch: 14\n",
      "Train accuracy:  0.19458483754512634\n",
      "Valid accuracy:  0.1691919191919192\n",
      "Epoch: 15\n",
      "Train accuracy:  0.1956678700361011\n",
      "Valid accuracy:  0.16835016835016836\n",
      "Epoch: 16\n",
      "Train accuracy:  0.20469314079422382\n",
      "Valid accuracy:  0.17845117845117844\n",
      "Epoch: 17\n",
      "Train accuracy:  0.21263537906137184\n",
      "Valid accuracy:  0.18181818181818182\n",
      "Epoch: 18\n",
      "Train accuracy:  0.21660649819494585\n",
      "Valid accuracy:  0.18602693602693604\n",
      "Epoch: 19\n",
      "Train accuracy:  0.2111913357400722\n",
      "Valid accuracy:  0.18602693602693604\n",
      "Epoch: 20\n",
      "Train accuracy:  0.23068592057761733\n",
      "Valid accuracy:  0.19276094276094277\n",
      "Epoch: 21\n",
      "Train accuracy:  0.2259927797833935\n",
      "Valid accuracy:  0.18855218855218855\n",
      "Epoch: 22\n",
      "Train accuracy:  0.23862815884476535\n",
      "Valid accuracy:  0.1936026936026936\n",
      "Epoch: 23\n",
      "Train accuracy:  0.24259927797833936\n",
      "Valid accuracy:  0.19023569023569023\n",
      "Epoch: 24\n",
      "Train accuracy:  0.24115523465703972\n",
      "Valid accuracy:  0.1936026936026936\n",
      "Epoch: 25\n",
      "Train accuracy:  0.2552346570397112\n",
      "Valid accuracy:  0.19444444444444445\n",
      "Epoch: 26\n",
      "Train accuracy:  0.2555956678700361\n",
      "Valid accuracy:  0.20286195286195285\n",
      "Epoch: 27\n",
      "Train accuracy:  0.25776173285198556\n",
      "Valid accuracy:  0.20033670033670034\n",
      "Epoch: 28\n",
      "Train accuracy:  0.2682310469314079\n",
      "Valid accuracy:  0.19696969696969696\n",
      "Epoch: 29\n",
      "Train accuracy:  0.2696750902527076\n",
      "Valid accuracy:  0.20875420875420875\n",
      "Epoch: 30\n",
      "Train accuracy:  0.26895306859205775\n",
      "Valid accuracy:  0.20033670033670034\n",
      "Epoch: 31\n",
      "Train accuracy:  0.26534296028880866\n",
      "Valid accuracy:  0.20033670033670034\n",
      "Epoch: 32\n",
      "Train accuracy:  0.2736462093862816\n",
      "Valid accuracy:  0.20622895622895623\n",
      "Epoch: 33\n",
      "Train accuracy:  0.27906137184115526\n",
      "Valid accuracy:  0.1978114478114478\n",
      "Epoch: 34\n",
      "Train accuracy:  0.28808664259927796\n",
      "Valid accuracy:  0.20202020202020202\n",
      "Epoch: 35\n",
      "Train accuracy:  0.28447653429602887\n",
      "Valid accuracy:  0.20117845117845118\n",
      "Epoch: 36\n",
      "Train accuracy:  0.28447653429602887\n",
      "Valid accuracy:  0.1978114478114478\n",
      "Epoch: 37\n",
      "Train accuracy:  0.30144404332129965\n",
      "Valid accuracy:  0.21212121212121213\n",
      "Epoch: 38\n",
      "Train accuracy:  0.30180505415162456\n",
      "Valid accuracy:  0.21212121212121213\n",
      "Epoch: 39\n",
      "Train accuracy:  0.29783393501805056\n",
      "Valid accuracy:  0.2079124579124579\n",
      "Epoch: 40\n",
      "Train accuracy:  0.3148014440433213\n",
      "Valid accuracy:  0.2112794612794613\n",
      "Epoch: 41\n",
      "Train accuracy:  0.3064981949458484\n",
      "Valid accuracy:  0.21043771043771045\n",
      "Epoch: 42\n",
      "Train accuracy:  0.3115523465703971\n",
      "Valid accuracy:  0.20286195286195285\n",
      "Epoch: 43\n",
      "Train accuracy:  0.3104693140794224\n",
      "Valid accuracy:  0.20875420875420875\n",
      "Epoch: 44\n",
      "Train accuracy:  0.3259927797833935\n",
      "Valid accuracy:  0.21464646464646464\n",
      "Epoch: 45\n",
      "Train accuracy:  0.3288808664259928\n",
      "Valid accuracy:  0.21296296296296297\n",
      "Epoch: 46\n",
      "Train accuracy:  0.32057761732851986\n",
      "Valid accuracy:  0.2053872053872054\n",
      "Epoch: 47\n",
      "Train accuracy:  0.3371841155234657\n",
      "Valid accuracy:  0.20959595959595959\n",
      "Epoch: 48\n",
      "Train accuracy:  0.33501805054151623\n",
      "Valid accuracy:  0.20622895622895623\n",
      "Epoch: 49\n",
      "Train accuracy:  0.3368231046931408\n",
      "Valid accuracy:  0.21296296296296297\n",
      "Epoch: 50\n",
      "Train accuracy:  0.35379061371841153\n",
      "Valid accuracy:  0.21801346801346802\n",
      "Epoch: 51\n",
      "Train accuracy:  0.35018050541516244\n",
      "Valid accuracy:  0.20875420875420875\n",
      "Epoch: 52\n",
      "Train accuracy:  0.3498194945848375\n",
      "Valid accuracy:  0.21632996632996632\n",
      "Epoch: 53\n",
      "Train accuracy:  0.3595667870036101\n",
      "Valid accuracy:  0.21632996632996632\n",
      "Epoch: 54\n",
      "Train accuracy:  0.36570397111913355\n",
      "Valid accuracy:  0.21043771043771045\n",
      "Epoch: 55\n",
      "Train accuracy:  0.36425992779783395\n",
      "Valid accuracy:  0.2138047138047138\n",
      "Epoch: 56\n",
      "Train accuracy:  0.36859205776173287\n",
      "Valid accuracy:  0.2255892255892256\n",
      "Epoch: 57\n",
      "Train accuracy:  0.3689530685920578\n",
      "Valid accuracy:  0.20875420875420875\n",
      "Epoch: 58\n",
      "Train accuracy:  0.3671480144404332\n",
      "Valid accuracy:  0.21296296296296297\n",
      "Epoch: 59\n",
      "Train accuracy:  0.3783393501805054\n",
      "Valid accuracy:  0.20959595959595959\n",
      "Epoch: 60\n",
      "Train accuracy:  0.37581227436823106\n",
      "Valid accuracy:  0.21717171717171718\n",
      "Epoch: 61\n",
      "Train accuracy:  0.3877256317689531\n",
      "Valid accuracy:  0.20959595959595959\n",
      "Epoch: 62\n",
      "Train accuracy:  0.3913357400722022\n",
      "Valid accuracy:  0.21212121212121213\n",
      "Epoch: 63\n",
      "Train accuracy:  0.39855595667870036\n",
      "Valid accuracy:  0.2138047138047138\n",
      "Epoch: 64\n",
      "Train accuracy:  0.3953068592057762\n",
      "Valid accuracy:  0.2196969696969697\n",
      "Epoch: 65\n",
      "Train accuracy:  0.40252707581227437\n",
      "Valid accuracy:  0.21464646464646464\n",
      "Epoch: 66\n",
      "Train accuracy:  0.40180505415162454\n",
      "Valid accuracy:  0.21717171717171718\n",
      "Epoch: 67\n",
      "Train accuracy:  0.4227436823104693\n",
      "Valid accuracy:  0.21885521885521886\n",
      "Epoch: 68\n",
      "Train accuracy:  0.4108303249097473\n",
      "Valid accuracy:  0.22138047138047137\n",
      "Epoch: 69\n",
      "Train accuracy:  0.41949458483754515\n",
      "Valid accuracy:  0.20622895622895623\n",
      "Epoch: 70\n",
      "Train accuracy:  0.4296028880866426\n",
      "Valid accuracy:  0.22138047138047137\n",
      "Epoch: 71\n",
      "Train accuracy:  0.4299638989169675\n",
      "Valid accuracy:  0.21801346801346802\n",
      "Epoch: 72\n",
      "Train accuracy:  0.4339350180505415\n",
      "Valid accuracy:  0.21548821548821548\n",
      "Epoch: 73\n",
      "Train accuracy:  0.42418772563176893\n",
      "Valid accuracy:  0.21885521885521886\n",
      "Epoch: 74\n",
      "Train accuracy:  0.4299638989169675\n",
      "Valid accuracy:  0.2196969696969697\n",
      "Epoch: 75\n",
      "Train accuracy:  0.4389891696750903\n",
      "Valid accuracy:  0.21464646464646464\n",
      "Epoch: 76\n",
      "Train accuracy:  0.43176895306859203\n",
      "Valid accuracy:  0.21717171717171718\n",
      "Epoch: 77\n",
      "Train accuracy:  0.44584837545126355\n",
      "Valid accuracy:  0.23063973063973064\n",
      "Epoch: 78\n",
      "Train accuracy:  0.4545126353790614\n",
      "Valid accuracy:  0.22474747474747475\n",
      "Epoch: 79\n",
      "Train accuracy:  0.4490974729241877\n",
      "Valid accuracy:  0.21717171717171718\n",
      "Epoch: 80\n",
      "Train accuracy:  0.45018050541516247\n",
      "Valid accuracy:  0.21464646464646464\n",
      "Epoch: 81\n",
      "Train accuracy:  0.44548736462093863\n",
      "Valid accuracy:  0.2281144781144781\n",
      "Epoch: 82\n",
      "Train accuracy:  0.463898916967509\n",
      "Valid accuracy:  0.22138047138047137\n",
      "Epoch: 83\n",
      "Train accuracy:  0.45776173285198557\n",
      "Valid accuracy:  0.20959595959595959\n",
      "Epoch: 84\n",
      "Train accuracy:  0.4696750902527076\n",
      "Valid accuracy:  0.22306397306397308\n",
      "Epoch: 85\n",
      "Train accuracy:  0.47581227436823104\n",
      "Valid accuracy:  0.22053872053872053\n",
      "Epoch: 86\n",
      "Train accuracy:  0.4750902527075812\n",
      "Valid accuracy:  0.2222222222222222\n",
      "Epoch: 87\n",
      "Train accuracy:  0.4732851985559567\n",
      "Valid accuracy:  0.23063973063973064\n",
      "Epoch: 88\n",
      "Train accuracy:  0.48014440433212996\n",
      "Valid accuracy:  0.21717171717171718\n",
      "Epoch: 89\n",
      "Train accuracy:  0.4732851985559567\n",
      "Valid accuracy:  0.22053872053872053\n",
      "Epoch: 90\n",
      "Train accuracy:  0.4747292418772563\n",
      "Valid accuracy:  0.21885521885521886\n",
      "Epoch: 91\n",
      "Train accuracy:  0.5072202166064982\n",
      "Valid accuracy:  0.23316498316498316\n",
      "Epoch: 92\n",
      "Train accuracy:  0.47581227436823104\n",
      "Valid accuracy:  0.21801346801346802\n",
      "Epoch: 93\n",
      "Train accuracy:  0.5046931407942238\n",
      "Valid accuracy:  0.23653198653198654\n",
      "Epoch: 94\n",
      "Train accuracy:  0.4927797833935018\n",
      "Valid accuracy:  0.23148148148148148\n",
      "Epoch: 95\n",
      "Train accuracy:  0.4981949458483754\n",
      "Valid accuracy:  0.23063973063973064\n",
      "Epoch: 96\n",
      "Train accuracy:  0.49927797833935017\n",
      "Valid accuracy:  0.22895622895622897\n",
      "Epoch: 97\n",
      "Train accuracy:  0.5140794223826715\n",
      "Valid accuracy:  0.2382154882154882\n",
      "Epoch: 98\n",
      "Train accuracy:  0.5018050541516246\n",
      "Valid accuracy:  0.2297979797979798\n",
      "Epoch: 99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.523826714801444\n",
      "Valid accuracy:  0.2297979797979798\n",
      "Test accuracy:  0.08104738154613467\n"
     ]
    }
   ],
   "source": [
    "network_mfcc = Sequential()\n",
    "sgd = torch.optim.Adam(network_mfcc.parameters(), lr=0.0001)\n",
    "\n",
    "train_losses, valid_accs = run_experiment(network_mfcc, train_data_loader, valid_data_loader, test_data_loader, sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectogram: IGNORE UNUSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1,out_channels=10,kernel_size=2,stride=1,padding=0)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=10, out_channels=32, kernel_size=2, stride=1, padding=0)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(in_features=32*2*11,out_features=100)\n",
    "        self.drop1 = torch.nn.Dropout(0.2)\n",
    "        self.fc2 = torch.nn.Linear(in_features=100,out_features=50)\n",
    "        self.drop2 = torch.nn.Dropout(0.2)\n",
    "        self.fc3 = torch.nn.Linear(in_features=50,out_features=20)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size = 64\n",
    "        X = self.conv1(X)\n",
    "        X = self.pool1(X)\n",
    "        X = self.conv2(X)\n",
    "        X = self.pool2(X)\n",
    "        X = X.relu()\n",
    "        X = X.view(batch_size, -1)\n",
    "        X = self.fc1(X)\n",
    "        X - self.drop1(X)\n",
    "        X = X.relu()\n",
    "        X = self.fc2(X)\n",
    "        X - self.drop2(X)\n",
    "        X = X.relu()\n",
    "        X = self.fc3(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Sequential(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.network=nn.Sequential(\n",
    "#             nn.Linear(40,128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(128, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(256, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(512,64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(64,20),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "#     def forward(self, X):\n",
    "#         return self.network(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float)\n",
    "X_train_tensor = X_train_tensor.reshape([X_train.shape[0],1,X_train.shape[1],X_train.shape[2]])\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float)\n",
    "X_test_tensor = X_test_tensor.reshape([X_test.shape[0],1,X_test.shape[1],X_test.shape[2]])\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=64,shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Convolutional()\n",
    "sgd = torch.optim.SGD(network.parameters(), lr=0.001)\n",
    "\n",
    "train_losses, valid_accs = run_experiment(network, train_data_loader, valid_data_loader, sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly drop some to make batch sizes even\n",
    "# drop = random.sample(range(0,X_train.shape[0]),4608)\n",
    "# X_train=X_train[list(drop)]\n",
    "# y_train=y_train[list(drop)]\n",
    "\n",
    "# drop = random.sample(range(0,X_test.shape[0]),1984)\n",
    "# X_test=X_test[list(drop)]\n",
    "# y_test=y_test[list(drop)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unused Section: Leave just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused\n",
    "def extract_features(signal, sr, n_fft, hop_length, n_mfcc):    \n",
    "    # Extract short-time fourier transform\n",
    "    stft = librosa.core.stft(signal, hop_length=hop_length, n_fft=n_fft)\n",
    "    \n",
    "    # Extract log spectrogram\n",
    "    log_spectrogram = extract_spectrogram(stft)\n",
    "    \n",
    "    # Extract MFCC\n",
    "    MFCC = librosa.feature.mfcc(signal, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc)\n",
    "    \n",
    "    # Extract chromagram\n",
    "    chromagram = librosa.feature.chroma_stft(signal, sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    # Extract harmonics and percussion\n",
    "    harmonics, percussion = extract_harmonics_percussion(stft)\n",
    "    \n",
    "    return log_spectrogram, MFCC, chromagram, harmonics, percussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spectrogram(stft):\n",
    "    spectrogram = np.abs(stft)\n",
    "    log_spectrogram = librosa.amplitude_to_db(spectrogram) #amplitude as a function of time and frequency\n",
    "    \n",
    "    return log_spectrogram\n",
    "\n",
    "def extract_harmonics_percussion(stft):\n",
    "    harm, perc = librosa.decompose.hpss(stft)\n",
    "    harm = librosa.amplitude_to_db(np.abs(harm))\n",
    "    perc = librosa.amplitude_to_db(np.abs(perc))\n",
    "    \n",
    "    return harm, perc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
